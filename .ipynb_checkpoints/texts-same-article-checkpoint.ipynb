{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9731541b-938e-4268-82c8-b12e676c37b3",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Texts Common Origin Analysis</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            The present project aims at conceiving a ML system designed for verifying whether two pieces of texts actually came from the same article.\n",
    "        </li>\n",
    "        <li>\n",
    "            The corpus we are dealing with has been extracted from Wikipedia by Professor Jeff Heaton and published at <a href='https://www.kaggle.com/datasets/jeffheaton/are-two-sentences-of-the-same-topic'>Kaggle</a>.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3603e2b-d0ae-4156-89ab-f3d374edebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>same_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>June – Moctezuma II, Aztec ruler of Tenochtitl...</td>\n",
       "      <td>The Swedish regent Sten Sture the Younger is m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The population was 1,097 at the 2010 census.</td>\n",
       "      <td>Like other Latino neighborhoods in New York Ci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europe and the Islamic World: A History.</td>\n",
       "      <td>There are no plans to resurrect it.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even where only a small charge is produced, it...</td>\n",
       "      <td>The Clarion-Limestone Area School District pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The highlight of Croatias recent infrastructur...</td>\n",
       "      <td>The closest analogy with the modern Web browse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent1  \\\n",
       "0  June – Moctezuma II, Aztec ruler of Tenochtitl...   \n",
       "1       The population was 1,097 at the 2010 census.   \n",
       "2           Europe and the Islamic World: A History.   \n",
       "3  Even where only a small charge is produced, it...   \n",
       "4  The highlight of Croatias recent infrastructur...   \n",
       "\n",
       "                                               sent2  same_source  \n",
       "0  The Swedish regent Sten Sture the Younger is m...            1  \n",
       "1  Like other Latino neighborhoods in New York Ci...            0  \n",
       "2                There are no plans to resurrect it.            0  \n",
       "3  The Clarion-Limestone Area School District pro...            0  \n",
       "4  The closest analogy with the modern Web browse...            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Altering pandas' default maximum column width.\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Loading the file.\n",
    "df = pd.read_csv('data/train.csv').drop('id', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac2c6d-8975-40ae-99f1-00d47511a2ab",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Dataset Overview</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Now let's explore the texts - especially those from the same articles - so that we can think about a sensible strategy to succeed in this challenge.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc541688-bce6-4840-94c7-9deabbcf2072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "same_source\n",
       "1    0.500488\n",
       "0    0.499512\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we are dealing with a well balanced dataset.\n",
    "df['same_source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4155a29a-aeb0-4476-b8e9-1ba6b589a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I've considered convenient to create a function that prints out\n",
    "# instances from each target.\n",
    "from pyboxen import boxen\n",
    "from typing import Literal\n",
    "\n",
    "def boxen_samples(df:pd.DataFrame, target:Literal[0, 1], sample_size:int=5, random_size:int=42)->None:\n",
    "    '''\n",
    "        Prints out some pairs of texts from a certain target in a `pyboxen.boxen` format.\n",
    "\n",
    "        Note: Texts from the positive class are displayed inside green boxes, whereas the ones\n",
    "        from the negative target in red ones.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `df`: `pd.DataFrame`\n",
    "            The project's dataset.\n",
    "        `target`: Literal[0, 1]\n",
    "            The target group from which to make samples.\n",
    "        `sample_size`: int, defaults to 5\n",
    "            The amount of instances to print.\n",
    "        `random_state`: int, defaults to 42\n",
    "            The sampling random state.\n",
    "    '''\n",
    "    color = 'green' if target == 1 else 'red' # Defining the boxen's color according to the wished target.\n",
    "    df_sample = df[df['same_source']==target].sample(sample_size, random_state=random_size)\n",
    "\n",
    "    for i, series in df_sample.iterrows():\n",
    "        print(boxen(f'Sentence 1: {series[\"sent1\"]} \\n\\nSentence 2: {series[\"sent2\"]}', title=f'Row {i}', color=color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db51884-7158-4f75-abaf-40818d7a54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m╭─\u001b[0m\u001b[32m Row 97148 \u001b[0m\u001b[32m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
      "\u001b[32m│\u001b[0mSentence 1: The racial makeup of the town was 97.56% White, 0.41% African American, 0.18% Native American, 0.41% \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0mAsian, 0.28% from other races, and 1.15% from two or more races.                                                 \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0mSentence 2: Originally Plantation Number 9 by the Court of Massachusetts Bay, Huntington has a colorful history, \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0mhinted at by the towns incorporation date of March 5, 1855, decades later than the towns around it.              \u001b[32m│\u001b[0m\n",
      "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m╭─\u001b[0m\u001b[32m Row 107031 \u001b[0m\u001b[32m───────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
      "\u001b[32m│\u001b[0m Sentence 1: Camelot is a castle and court associated with the legendary King Arthur.                            \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m Sentence 2: The name of the Romano-British town of Camulodunum (modern Colchester) was derived from the Celtic  \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m god Camulus.                                                                                                    \u001b[32m│\u001b[0m\n",
      "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m╭─\u001b[0m\u001b[32m Row 6853 \u001b[0m\u001b[32m─────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
      "\u001b[32m│\u001b[0mSentence 1: Celestial historian Richard Allen noted that unlike the other constellations introduced by Plancius  \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0mand La Caille, Phoenix has actual precedent in ancient astronomy, as the Arabs saw this formation as representing\u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0myoung ostriches, Al Riāl, or as a griffin or eagle.                                                              \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0mSentence 2: BD is of spectral type A1V, and ranges between magnitudes 5.90 and 5.94.                             \u001b[32m│\u001b[0m\n",
      "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m╭─\u001b[0m\u001b[32m Row 5112 \u001b[0m\u001b[32m────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m                             \n",
      "\u001b[32m│\u001b[0mSentence 1: Hispanic or Latino of any race were 0.81% of the population.            \u001b[32m│\u001b[0m                             \n",
      "\u001b[32m│\u001b[0m                                                                                    \u001b[32m│\u001b[0m                             \n",
      "\u001b[32m│\u001b[0mSentence 2: There were 87 housing units at an average density of 2.4/sqmi (0.9/km²).\u001b[32m│\u001b[0m                             \n",
      "\u001b[32m╰────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m                             \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m╭─\u001b[0m\u001b[32m Row 24352 \u001b[0m\u001b[32m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
      "\u001b[32m│\u001b[0m Sentence 1: The population density was 4,519.4 people per square mile (1,621.6/km²).                            \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m Sentence 2: It is bordered to the west by Goose Creek, to the northwest by Barbourmeade, to the north by Manor  \u001b[32m│\u001b[0m\n",
      "\u001b[32m│\u001b[0m Creek, to the east by Ten Broeck, and to the south by a portion of Louisville.                                  \u001b[32m│\u001b[0m\n",
      "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instances from the positive class.\n",
    "boxen_samples(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021ea065-0960-4503-b41a-5cc897f3c5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m╭─\u001b[0m\u001b[31m Row 48222 \u001b[0m\u001b[31m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0mSentence 1: In addition, it is to be noted there was a 1-month period in which another woman served as the Acting\u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0mPostmaster for Gratz during Ms. Suters tenure, perhaps due to a short leave of absence for Suter and, that acting\u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0mrole was held by Roberta G Minish from December 21, 1927 until January 18, 1928 when Ms. Suter returned to her   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0mposition as Postmaster.                                                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0mSentence 2: At the same time, the Chinese army of Ganzhou reconquers Turpan in Northern Xiongnu.                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m╭─\u001b[0m\u001b[31m Row 90996 \u001b[0m\u001b[31m───────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m                                             \n",
      "\u001b[31m│\u001b[0mSentence 1: Sid and Nancy (1986), by Alex Cox.                      \u001b[31m│\u001b[0m                                             \n",
      "\u001b[31m│\u001b[0m                                                                    \u001b[31m│\u001b[0m                                             \n",
      "\u001b[31m│\u001b[0mSentence 2: South New Castle is located at  (40.975430, -80.344624).\u001b[31m│\u001b[0m                                             \n",
      "\u001b[31m╰────────────────────────────────────────────────────────────────────╯\u001b[0m                                             \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m╭─\u001b[0m\u001b[31m Row 86016 \u001b[0m\u001b[31m────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m                                                        \n",
      "\u001b[31m│\u001b[0mSentence 1: Hard fall for man who had it all.            \u001b[31m│\u001b[0m                                                        \n",
      "\u001b[31m│\u001b[0m                                                         \u001b[31m│\u001b[0m                                                        \n",
      "\u001b[31m│\u001b[0mSentence 2: The population was 11,545 at the 2010 census.\u001b[31m│\u001b[0m                                                        \n",
      "\u001b[31m╰─────────────────────────────────────────────────────────╯\u001b[0m                                                        \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m╭─\u001b[0m\u001b[31m Row 15535 \u001b[0m\u001b[31m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m  Sentence 1: For every 100 females, there were 96.3 males.                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m  Sentence 2: The median income for a household in the CDP was $24,679, and the median income for a family was   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m  $31,719.                                                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m╭─\u001b[0m\u001b[31m Row 29459 \u001b[0m\u001b[31m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m  Sentence 1: For every 100 females, there were 118.3 males.                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m  Sentence 2: As of the census of 2000, there were 226 people, 96 households, and 67 families residing in the    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m  township.                                                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instances from the negative class.\n",
    "boxen_samples(df, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b743ac1-f366-4cd6-859b-dc4c717360fa",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sample Analysis</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            We can observe that most fragments sampled from the same article touches on similar topics (demographics, history), and so tend to resort to typical words from that knowledge area.\n",
    "        </li>\n",
    "        <li>\n",
    "            But we need to be aware that sometimes such documents may use jargons that are very specific to their field (\"A1V\"). A model might not be able to reckognize such term-theme relation if the word is rare in the corpus.\n",
    "        </li>\n",
    "        <li>\n",
    "            And at last, note that there are some texts that although appear to be of the same subject, they do not belong to the same article! Just look at the last two red boxes that talk about with demographics. Since correctly classifying those cases would be much more intricate even for a human, we need to accept a certain percentage of errors from our system.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7687d68-4398-4a77-835e-a3b1d31430de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2 style='font-size:30px'> Strategy Explanation</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Considering that same article fragments tend to discuss similar topics, I think that proceeding a topic modeling would highly suit our mission.\n",
    "        </li>\n",
    "        <li>\n",
    "            We'd do so by running any well-known method (NMF or Latent Dirichlet Allocation) over our bag-of-words matrix. Then, when given a pair of texts, we would compare the topic association scores between them to conclude whether they originated from the same document. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4da01-2d2a-4477-bdc6-231fbb1e2947",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Data Treatment</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Before attempting any solution, we must turn our corpus into a matrix so that we can apply ML techniques.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a1c61-5671-47e1-935c-473e374c0df3",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Dataset Split</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            In order to legitimize any quality conclusion about our study, we must separate an exclusive dataset for tests.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5156bac1-82a7-44a1-9446-792dc98c2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], train_size=.8, stratify=df.iloc[:, -1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "adf1fd99-ea42-4511-9774-487c67fb275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import test_proportions_2indep\n",
    "\n",
    "def _assert_equal_prop(y:pd.Series, alpha:float=.05)->bool:\n",
    "    '''\n",
    "        Verifies whether the sets produced by `sklearn.model_selection.train_test_split` have a \n",
    "        target distribution similar to the original dataset.\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        `y`: `pd.Series`\n",
    "            The target distribution of a given dataset.\n",
    "        `alpha`: float\n",
    "            The significance level of our hypothesis test.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A boolean signaling if the distributions are statistically similar.\n",
    "    '''\n",
    "    y_counts = y.value_counts()\n",
    "    df_counts = df['same_source'].value_counts()\n",
    "    pvalue = test_proportions_2indep(df_counts[1], df_counts.sum(), y_counts[1], y_counts.sum()).pvalue\n",
    "    if pvalue<alpha:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "39e72549-4763-450d-8184-91579749e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions are not statistically similar\n"
     ]
    }
   ],
   "source": [
    "# The Hypothesis Tests assert that the split successfully maintained similar target distributions\n",
    "# along the datasets.\n",
    "try:\n",
    "    _assert_equal_prop(y_train)\n",
    "    _assert_equal_prop(y_test)\n",
    "    print('Distributions Ok')\n",
    "except:\n",
    "    print('Distributions are not statistically similar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141fbee-a4e2-47b8-ab5f-886bd19589a2",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Vectorizing the Texts</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            To make possible the use of any ML technique over our data, we must turn it into a matrix. In our case, I'll apply an standard TF-IDF.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c52ed6e3-9cbd-4afd-85ca-3dfced5e8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd6fad0b-cd61-42e2-af2f-a4f2bdc2e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from typing import List\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    '''\n",
    "        Lemmatizer to be used as the `tokenizer` argument in the \n",
    "        `sklearn.feature_extraction.text.TfidfVectorizer` class. It tokenizes  the string and \n",
    "        applies lemmatization, according to the WordNet Pos-Tagging.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        '''\n",
    "            Converts a Tree Bank TAG into a WordNet TAG.\n",
    "\n",
    "            Parameter\n",
    "            ---------\n",
    "            `treebank_tag`: str\n",
    "                The Tree Bank TAG\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            The converted TAG.`\n",
    "        '''\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    def __call__(self, doc)->List[str]:\n",
    "        tokens = word_tokenize(doc)\n",
    "        tokens_tags = pos_tag(tokens)\n",
    "        return [self.wnl.lemmatize(token, pos=self.get_wordnet_pos(pos)) for token, pos in tokens_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e67cd4c-0b2c-41fa-9571-5f217a1b80d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Retrieved on December 20, 2008.',\n",
       "       'The population density was 18 people per square mile (7/km²).',\n",
       "       'In contrast, successors to the illustrative approach, such as Gil Kane, found their work eventually reach an impasse.',\n",
       "       ...,\n",
       "       'Louis also had to abandon claims on fiefdoms in Mecklenburg and Pomerania.',\n",
       "       'Hispanic or Latino of any race were 2.69% of the population.',\n",
       "       'Their expertise is in the examination of evidence or relevant facts in the case.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It would be benefitial to leverage the texts from both columns to carry out the topic modeling.\n",
    "train_texts = np.concatenate((X_train.iloc[:, 0].to_numpy(), X_train.iloc[:, 1].to_numpy()))\n",
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d6a7d0bc-8e0b-4bc5-860b-74a7b2453d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# tf_idf = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words, strip_accents='ascii')\n",
    "# X_train_lda = tf_idf.fit_transform(train_texts)\n",
    "save_npz('data/train-lda.npz', X_train_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3893032a-7952-4ed4-933f-2951c42b24c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 9509509] Fitar o TF-IDF com o conteúdo de ambas as colunas\n",
      " 4 files changed, 485 insertions(+), 65 deletions(-)\n",
      "Enumerating objects: 11, done.\n",
      "Counting objects: 100% (11/11), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (6/6), done.\n",
      "Writing objects: 100% (6/6), 7.20 KiB | 7.20 MiB/s, done.\n",
      "Total 6 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/texts-same-article.git\n",
      "   c54b2aa..9509509  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m 'Rodar LDA'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7251614-9e9e-47f6-9485-b4e582e7b03b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Expliquei estratégia do projeto; Vetorizei set de treino; Rodar LDA</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texts-same-article",
   "language": "python",
   "name": "texts-same-article"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
